{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid_search\n",
    "\n",
    "**Short Introduction** \n",
    "\n",
    "This notebook allows to perform parameter grid searches. For every type of pipeline there is a corresponding grid search config. In this config, the parameter ranges can be configures. The grid search is then performed and the results are saved to a log file.\n",
    "The number of workers should be chosen according to the number of available CPU cores and the memory consumption of the pipeline. Pipeline functions have been optimized for concurrency to limit the memory consumption and use cached data when used repeatedly. On DDR4 systems RAM speed can become a bottleneck when using compute intensive regressors.\n",
    "\n",
    "---\n",
    "The `execute_pipeline` function manages **parallel execution** of a pipeline function (e.g., a hyperparameter sweep) over multiple parameter combinations. It uses a **ProcessPoolExecutor** to run jobs concurrently, and **logs results to CSV** in consistent columns. The function also includes **basic error tracking** and can handle large workloads in a batched manner to reduce file I/O overhead.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "- **pipeline_function**  \n",
    "  The function to apply to each parameter combination. Generally, this is a pipeline or model evaluation function.\n",
    "\n",
    "- **param_combinations** (`list`)  \n",
    "  A list of tuples (or lists) representing the different combinations of parameters to evaluate.\n",
    "\n",
    "- **param_keys** (`list` of `str`)  \n",
    "  Names corresponding to each position in the parameter combinations, ensuring each combination can be unpacked into a dictionary.\n",
    "\n",
    "- **log_path** (`str`)  \n",
    "  File path to which results are written as CSV. Automatically creates the parent directory if needed.\n",
    "\n",
    "- **num_workers** (`int`, default=6)  \n",
    "  Maximum number of worker processes for parallel execution. Limited by the number of CPU threads available.\n",
    "\n",
    "- **data_cache** (any, optional)  \n",
    "  Shared data or resources to be passed into each pipeline call if needed.\n",
    "\n",
    "- **batch_size** (`int`, default=100)  \n",
    "  Number of results to buffer in memory before writing to the CSV file in bulk.\n",
    "\n",
    "---\n",
    "\n",
    "## Return Values\n",
    "\n",
    "**No direct return**. The function writes results to a CSV at `log_path`. It also  \n",
    "- Tracks the number of successful vs. failed runs  \n",
    "- Logs the most frequently occurring error message  \n",
    "- Optionally sends progress and completion notifications  \n",
    "\n",
    "Under the hood, it organizes the final CSV so that columns appear in a fixed, consistent order, even if new metrics or errors appear in later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many workers are available\n",
    "import os\n",
    "print(f'Available workers: {os.cpu_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Efficiency Map Config\n",
    "\n",
    "The following cell contains the configuration to perform a grid search for the efficiency map pipeline. The First part of the config dictionary should not be changed, only the param_values key should be adjusted to the desired parameter ranges. For every parameter a list of values should be provided. If a parameter shall not be varied, a list with a single value should be provided. Specify a comment to make the current grid search more unique in the log file. For this pipeline using certain `twoD_smoothing_kwargs` like specific regressor configs or other lazy evaluation models can lead to excessive memory usage. Reduce accordingly if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from modules.parametric_pipelines import efficiencymap_pipeline\n",
    "from modules.threadpool_executer import execute_pipeline\n",
    "from modules.data_handler import get_can_files\n",
    "\n",
    "\n",
    "# Define the parameter space for Efficiency Map Pipeline\n",
    "efficiencymap_config = {\n",
    "    \"comment\": \"Comment for the run\",\n",
    "    \"param_keys\": [\n",
    "        'comment', 'files', 'gear', 'efficiency_limit_lower', 'efficiency_limit_upper', 'soc_limit_lower', 'soc_limit_upper', 'remove_neutral_gear',\n",
    "        'smoothing_kwargs', 'columns_to_smooth', 'substract_auxiliary_power', 'which_full_load_curve', 'twoD_smoothing_kwargs', 'high_fidelity_interpolation',\n",
    "        'n_quantize_bins', 'at_middle_of_bin', 'n_interpolation_bins', 'global_offset', 'generate_plots', 'verbose'\n",
    "    ],\n",
    "    \"param_values\": list(product(\n",
    "        [\"Test Run\"],  # Supply comment\n",
    "        [   # files\n",
    "            get_can_files(folder='data/', exclude_keywords=['coastdown', 'sascha', 'RLC_Konstantfahrt'])\n",
    "            # or use 'all' to use all files\n",
    "        ],\n",
    "        [1, 2],  # gear\n",
    "        [0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4],  # efficiency_limit_lower\n",
    "        [0.97, 0.98, 0.99, 1],  # efficiency_limit_upper\n",
    "        [0],  # soc_limit_lower\n",
    "        [100],  # soc_limit_upper\n",
    "        [True],  # remove_neutral_gear\n",
    "        [  # smoothing_kwargs\n",
    "            {'filter_type': 'moving_average', 'window_size': 3},\n",
    "            {'filter_type': 'moving_average', 'window_size': 5},\n",
    "            {'filter_type': 'moving_average', 'window_size': 7},\n",
    "            {'filter_type': 'moving_average', 'window_size': 10},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.4},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.6},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.8},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 5, 'polyorder': 3},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 10, 'polyorder': 3},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 15, 'polyorder': 3},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 5, 'polyorder': 5},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 10, 'polyorder': 5},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 15, 'polyorder': 5},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 5, 'polyorder': 1},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 10, 'polyorder': 1},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 15, 'polyorder': 1},\n",
    "            None\n",
    "        ],\n",
    "        [   # columns_to_smooth\n",
    "            ['hv_battery_current', 'hv_battery_voltage', 'rear_motor_torque', 'engine_rpm', 'dcdc_power_hv'],\n",
    "            ['dcdc_power_hv', 'hv_battery_current', 'hv_battery_voltage'],\n",
    "            ['rear_motor_torque', 'engine_rpm']\n",
    "        ],\n",
    "        [True],  # substract_auxiliary_power\n",
    "        ['adjusted'],  # which_full_load_curve\n",
    "        [  # twoD_smoothing_kwargs\n",
    "            {  # IDW config\n",
    "                'method': 'idw',\n",
    "                'power': 1,\n",
    "                'num_neighbors': 10,\n",
    "                'outlier_detection': True,\n",
    "                'threshold_multiplier': 2\n",
    "            },\n",
    "            {  # Gaussian filter config\n",
    "                'method': 'gaussian_filter',\n",
    "                'sigma': 2,\n",
    "                'grid_size': 50\n",
    "            },\n",
    "            {  # Griddata config\n",
    "                'method': 'griddata',\n",
    "                'interp_method': 'nearest'\n",
    "            },\n",
    "            {  # Regression config\n",
    "                'method': 'regression',\n",
    "                'model': 'random_forest',\n",
    "                'model_params': {\n",
    "                    'n_estimators': 50,\n",
    "                    'max_depth': 5,\n",
    "                    'random_state': 42\n",
    "                }\n",
    "            },\n",
    "            None\n",
    "        ],\n",
    "        [False],  # high_fidelity_interpolation\n",
    "        [10],  # n_quantize_bins\n",
    "        [True],  # at_middle_of_bin\n",
    "        [10],  # n_interpolation_bins\n",
    "        [0],  # global_offset\n",
    "        [False],  # generate_plots (leave off for grid search)\n",
    "        [False]  # verbose (leave off for grid search)\n",
    "    )),\n",
    "    \"log_path\": \"data/logs/efficiencymap_log.csv\",\n",
    "    \"pipeline_function\": efficiencymap_pipeline\n",
    "}\n",
    "\n",
    "\n",
    "config = efficiencymap_config\n",
    "\n",
    "# Preload data to cache for disk I/O optimization\n",
    "def collect_all_filenames(param_values, param_keys):\n",
    "    from modules.data_handler import get_can_files\n",
    "    files_index = param_keys.index('files')\n",
    "    unique_files = set()\n",
    "    for params in param_values:\n",
    "        files_param = params[files_index]\n",
    "        if files_param == 'all':\n",
    "            return get_can_files()\n",
    "        else:\n",
    "            unique_files.update(files_param)\n",
    "    return list(unique_files)\n",
    "\n",
    "# Collect all files\n",
    "files_to_load = collect_all_filenames(config['param_values'], config['param_keys'])\n",
    "data_cache = {}\n",
    "from modules.data_handler import load_can_data\n",
    "for filename in files_to_load:\n",
    "    data = load_can_data(filename, verbose=False)\n",
    "    data_cache[filename] = data\n",
    "\n",
    "\n",
    "num_workers = 96 # Adjust to available workers\n",
    "# Execute the pipeline\n",
    "execute_pipeline(\n",
    "    config['pipeline_function'],\n",
    "    config['param_values'],\n",
    "    config['param_keys'],\n",
    "    config['log_path'],\n",
    "    num_workers=num_workers,\n",
    "    data_cache=data_cache,\n",
    "    batch_size=num_workers*2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Coastdown Config\n",
    "\n",
    "The following cell contains the configuration to perform a grid search for the coastdown pipeline. It works the same as the efficiency map config. The First part of the config dictionary should not be changed, only the param_values key should be adjusted to the desired parameter ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from modules.parametric_pipelines import coastdown_pipeline\n",
    "from modules.threadpool_executer import execute_pipeline\n",
    "from modules.data_handler import get_can_files, load_can_data\n",
    "import numpy as np\n",
    "\n",
    "path = 'data/path_to_coastdown_files'  # Adjust to your path\n",
    "files = get_can_files(folder=path)\n",
    "\n",
    "# Define the parameter space for Coastdown Pipeline\n",
    "coastdown_config = {\n",
    "    \"comment\": \"Coastdown Grid Search\",\n",
    "    \"param_keys\": [\n",
    "        'comment',\n",
    "        'files',\n",
    "        'do_pitch_correction',\n",
    "        'speed_signal',\n",
    "        'bucket_size',\n",
    "        'vehicle_mass',\n",
    "        'rotating_mass_eq',\n",
    "        'frontal_area',\n",
    "        'smoothing_kwargs',\n",
    "        'columns_to_smooth',\n",
    "        'steering_angle_limit',\n",
    "        'select_suspension_level',\n",
    "        'deriv_lower_limit',\n",
    "        'deriv_upper_limit',\n",
    "        'cut_time',\n",
    "        'seed',\n",
    "        'target_n_segments',\n",
    "        'outlier_threshold',\n",
    "        'loss_type',\n",
    "        'generate_plots',\n",
    "        'verbose'\n",
    "    ],\n",
    "    \"param_values\": list(product(\n",
    "        [\"Run 1\"],  # Supply comment\n",
    "        [   # files\n",
    "            files\n",
    "        ],\n",
    "        [True],  # do_pitch_correction\n",
    "        [   # speed_signal\n",
    "            'vehicle_speed',\n",
    "            'vehicle_speed_pitch_corrected',\n",
    "            'vehicle_speed_gps',\n",
    "            'vehicle_speed_gps_pitch_corrected'\n",
    "        ],\n",
    "        [2, 4, 6, 10, 15],  # bucket_size\n",
    "        [2300],  # vehicle_mass\n",
    "        [50],  # rotating_mass_eq\n",
    "        [2.33],  # frontal_area\n",
    "        [   # smoothing_kwargs\n",
    "            {'filter_type': 'moving_average', 'window_size': 5},\n",
    "            {'filter_type': 'moving_average', 'window_size': 20},\n",
    "            {'filter_type': 'moving_average', 'window_size': 50},\n",
    "            {'filter_type': 'moving_average', 'window_size': 100},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.03},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.05},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.1},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.2},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.4},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 5, 'polyorder': 3},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 25, 'polyorder': 3},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 50, 'polyorder': 3},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 100, 'polyorder': 3},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 5, 'polyorder': 5},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 25, 'polyorder': 5},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 50, 'polyorder': 5},\n",
    "            {'filter_type': 'savitzky_golay', 'window_length': 100, 'polyorder': 5},\n",
    "            None\n",
    "        ],\n",
    "        [   # columns_to_smooth\n",
    "            ['accelerator_pedal', 'steering_wheel_angle', 'vehicle_speed']\n",
    "        ],\n",
    "        [1, 3, 10],  # steering_angle_limit\n",
    "        [None],  # select_suspension_level\n",
    "        [-0.5, -0.4, -0.3, -0.2],  # deriv_lower_limit\n",
    "        [-0.01, 0, 0.01, 0.02, 0.03, 0.04],  # deriv_upper_limit\n",
    "        [1, 3, 5, 10],  # cut_time\n",
    "        [42],  # seed\n",
    "        [None],  # target_n_segments\n",
    "        [None, 0.5, 1],  # outlier_threshold\n",
    "        ['sec_rmse'],  # loss_type\n",
    "        [False],  # generate_plots\n",
    "        [False]  # verbose\n",
    "    )),\n",
    "    \"log_path\": \"data/logs/coastdown_log.csv\",\n",
    "    \"pipeline_function\": coastdown_pipeline\n",
    "}\n",
    "\n",
    "config = coastdown_config\n",
    "\n",
    "def collect_all_filenames(param_values, param_keys):\n",
    "    files_index = param_keys.index('files')\n",
    "    unique_files = set()\n",
    "    for params in param_values:\n",
    "        files_param = params[files_index]\n",
    "        if files_param == 'all':\n",
    "            unique_files.update(get_can_files())\n",
    "        else:\n",
    "            unique_files.update(files_param)\n",
    "    return list(unique_files)\n",
    "\n",
    "# Preload data to cache for disk I/O optimization\n",
    "files_to_load = collect_all_filenames(config['param_values'], config['param_keys'])\n",
    "data_cache = {}\n",
    "for filename in files_to_load:\n",
    "    data = load_can_data(filename, verbose=False)\n",
    "    data_cache[filename] = data\n",
    "\n",
    "num_workers = 52 # Adjust to available workers\n",
    "# Execute the pipeline with data_cache\n",
    "execute_pipeline(\n",
    "    config['pipeline_function'],\n",
    "    config['param_values'],\n",
    "    config['param_keys'],\n",
    "    config['log_path'],\n",
    "    num_workers=num_workers,\n",
    "    data_cache=data_cache,\n",
    "    batch_size=num_workers*2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Constspeed Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from modules.parametric_pipelines import constspeed_pipeline\n",
    "from modules.threadpool_executer import execute_pipeline\n",
    "from modules.data_handler import get_can_files, load_can_data\n",
    "\n",
    "files_neubiberg = get_can_files(folder='data/path_to_constant_speed_files')  # Adjust to your path\n",
    "\n",
    "# Define the parameter space for Constant Speed Pipeline\n",
    "constspeed_config = {\n",
    "    \"comment\": \"Constant Speed Grid Search\",\n",
    "    \"param_keys\": [\n",
    "        'comment',\n",
    "        'files',\n",
    "        'speed_signal',\n",
    "        'speed_threshold',\n",
    "        'min_n_samples',\n",
    "        'min_avg_speed',\n",
    "        'do_pitch_correction',\n",
    "        'cut_time',\n",
    "        'smoothing_kwargs',\n",
    "        'columns_to_smooth',\n",
    "        'steering_angle_limit',\n",
    "        'select_suspension_level',\n",
    "        'vehicle_mass',\n",
    "        'frontal_area',\n",
    "        'outlier_threshold',\n",
    "        'loss_type',\n",
    "        'generate_plots',\n",
    "        'verbose'\n",
    "    ],\n",
    "    \"param_values\": list(product(\n",
    "        [\"Run 1\"],  # comment\n",
    "        [files],  # files\n",
    "        [   # speed_signal\n",
    "            'vehicle_speed'\n",
    "        ],\n",
    "        [1, 2, 4, 8],  # speed_threshold\n",
    "        [10, 20, 40],  # min_n_samples\n",
    "        [2, 4, 10],  # min_avg_speed\n",
    "        [True],  # do_pitch_correction\n",
    "        [None, 1, 4, 8],  # cut_time\n",
    "        [   # smoothing_kwargs\n",
    "            {'filter_type': 'moving_average', 'window_size': 5},\n",
    "            {'filter_type': 'moving_average', 'window_size': 20},\n",
    "            {'filter_type': 'moving_average', 'window_size': 50},\n",
    "            {'filter_type': 'moving_average', 'window_size': 100},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.03},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.05},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.1},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.2},\n",
    "            {'filter_type': 'exponential_moving_average', 'alpha': 0.4},\n",
    "            None\n",
    "        ],\n",
    "        [   # columns_to_smooth\n",
    "            ['vehicle_speed']\n",
    "        ],\n",
    "        [1, 3, 5, 10],  # steering_angle_limit\n",
    "        [None],  # select_suspension_level\n",
    "        [2300],  # vehicle_mass\n",
    "        [2.33],  # frontal_area\n",
    "        [None, 0.5, 1],  # outlier_threshold\n",
    "        ['sec_rmse'],  # loss_type\n",
    "        [False],  # generate_plots\n",
    "        [False]  # verbose\n",
    "    )),\n",
    "    \"log_path\": \"data/logs/constspeed_pipeline_log_thesis.csv\",\n",
    "    \"pipeline_function\": constspeed_pipeline\n",
    "}\n",
    "\n",
    "config = constspeed_config\n",
    "\n",
    "def collect_all_filenames(param_values, param_keys):\n",
    "    files_index = param_keys.index('files')\n",
    "    unique_files = set()\n",
    "    for params in param_values:\n",
    "        files_param = params[files_index]\n",
    "        if files_param == 'all':\n",
    "            unique_files.update(get_can_files())\n",
    "        else:\n",
    "            unique_files.update(files_param)\n",
    "    return list(unique_files)\n",
    "\n",
    "# Preload data to cache for disk I/O optimization\n",
    "files_to_load = collect_all_filenames(config['param_values'], config['param_keys'])\n",
    "data_cache = {}\n",
    "for filename in files_to_load:\n",
    "    data = load_can_data(filename, verbose=False)\n",
    "    data_cache[filename] = data\n",
    "\n",
    "# Execute the pipeline with data_cache\n",
    "num_workers = 116 # Adjust to available workers\n",
    "execute_pipeline(\n",
    "    config['pipeline_function'],\n",
    "    config['param_values'],\n",
    "    config['param_keys'],\n",
    "    config['log_path'],\n",
    "    num_workers=num_workers,\n",
    "    data_cache=data_cache,\n",
    "    batch_size=num_workers*2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gear Strategy Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from modules.parametric_pipelines import gearstrategy_pipeline\n",
    "from modules.threadpool_executer import execute_pipeline\n",
    "from modules.data_handler import get_can_files, load_can_data\n",
    "import pickle\n",
    "\n",
    "# Define the parameter space for Gear Strategy Pipeline\n",
    "gearstrategy_config = {\n",
    "    \"comment\": \"Gear Strategy Grid Search\",\n",
    "    \"param_keys\": [\n",
    "        'comment',\n",
    "        'smoothing_kwargs',\n",
    "        'columns_to_smooth',\n",
    "        'gear_change_offset_samples',\n",
    "        'outlier_eps',\n",
    "        'outlier_min_samples',\n",
    "        'close_points_merge_thr',\n",
    "        'n_clusters',\n",
    "        'attach_endpoints',\n",
    "        'anchor_points',\n",
    "        'spline_order',\n",
    "        'num_knots',\n",
    "        'cluster_weight',\n",
    "        'normal_weight',\n",
    "        'knot_distr_method',\n",
    "        'verbose',\n",
    "        'generate_comparison_plots',\n",
    "        'generate_plots'\n",
    "    ],\n",
    "    \"param_values\": list(product(\n",
    "        [\"Run 1\"], # comment\n",
    "        [   # smoothing_kwargs\n",
    "            None\n",
    "        ],\n",
    "        [   # columns_to_smooth\n",
    "            None\n",
    "        ],\n",
    "        [0], # gear_change_offset_samples\n",
    "        [0.025, 0.05, 0.1, 0.125], # outlier_eps\n",
    "        [2, 3, 4], # outlier_min_samples\n",
    "        [0.0005, 0.001, 0.005, 0.01, 0.025, 0.05], # close_points_merge_thr\n",
    "        [5, 6, 7, 8], # n_clusters\n",
    "        [True, False], # attach_endpoints\n",
    "        [True], # anchor_points\n",
    "        [1, 2, 3], # spline_order\n",
    "        [3, 5, 7, 9], # num_knots\n",
    "        [0.1, 0.3, 0.5, 0.7, 0.9], # cluster_weight\n",
    "        [None], # normal_weight set to None to keep it inverse to cluster_weight\n",
    "        ['cluster'], # knot_distr_method\n",
    "        [False], # verbose\n",
    "        [False], # generate_comparison_plots\n",
    "        [False] # generate_plots\n",
    "    )),\n",
    "    \"log_path\": \"data/logs/gear_strategy_log.csv\",\n",
    "    \"pipeline_function\": gearstrategy_pipeline\n",
    "}\n",
    "\n",
    "config = gearstrategy_config\n",
    "\n",
    "# Initialize data_cache\n",
    "data_cache = {}\n",
    "\n",
    "# Load the ground truth results_normal dictionary\n",
    "with open('data/path_to_results_normal.pkl', 'rb') as f:\n",
    "    results_normal_loaded = pickle.load(f)\n",
    "\n",
    "# Load the results_sport dictionary\n",
    "with open('data/path_to_results_sport.pkl', 'rb') as f:\n",
    "    results_sport_loaded = pickle.load(f)\n",
    "\n",
    "# Remove 'line_function' from results dictionaries if present\n",
    "for axis2 in results_normal_loaded:\n",
    "    results_normal_loaded[axis2].pop('line_function', None)\n",
    "\n",
    "for axis2 in results_sport_loaded:\n",
    "    results_sport_loaded[axis2].pop('line_function', None)\n",
    "\n",
    "# Load the gear change data\n",
    "normal_files = get_can_files('data/path_to_files_recorded_in_normal_mode')  # Adjust to your path\n",
    "normal_data = load_can_data(normal_files[0], verbose=False)\n",
    "\n",
    "sport_files = get_can_files('data/path_to_files_recorded_in_sport_mode')  # Adjust to your path\n",
    "sport_data = load_can_data(sport_files[0], verbose=False)\n",
    "\n",
    "# Store loaded data into data_cache\n",
    "data_cache['results_normal_loaded'] = results_normal_loaded\n",
    "data_cache['results_sport_loaded'] = results_sport_loaded\n",
    "data_cache['normal_data'] = normal_data\n",
    "data_cache['sport_data'] = sport_data\n",
    "\n",
    "num_workers = 7 # Adjust to available workers\n",
    "# Execute the pipeline with data_cache\n",
    "execute_pipeline(\n",
    "    config['pipeline_function'],\n",
    "    config['param_values'],\n",
    "    config['param_keys'],\n",
    "    config['log_path'],\n",
    "    num_workers=num_workers,\n",
    "    data_cache=data_cache,\n",
    "    batch_size=num_workers*2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
