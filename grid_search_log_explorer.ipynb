{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid_search_log_explorer\n",
    "\n",
    "To quickly iterate on grid search parameter ranges, this notebook provides an interactive way to explore the log files generated by the `grid_search.ipynb` notebook. Various different settings can be made, which will be explained in the following sections. The structure of this log evaluation approach is also used in the `grid_search_thesis_plots.ipynb` notebook, which is used to generate the plots in chapter 4 of the thesis.\n",
    "\n",
    "---\n",
    "## Selection of algorithm, target metric, and log file\n",
    "To start out, one of the four different algorithms can be selected. Also required is to provide the desired target metric and wheather a high or low value is desired. Any of the metrics that are returned by the pipeline functions can be used as a target for the evaluation. Referr to the individual notebooks for an exhastive list. With a given log file path, the data can be loaded and the exploration can start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# select pipeline: uncomment one of the following blocks\n",
    "\n",
    "algorithm = 'coastdown'\n",
    "target_metric = 'loss_low_suspension' #\"loss_very_low_suspension\", \"loss_low_suspension\", \"loss_medium_suspension\", \"loss_high_suspension\", \"loss_CoC_values\", \"loss_EPA_values\", 'elapsed_time'\n",
    "higher_is_better = False\n",
    "\n",
    "# algorithm = 'constspeed'\n",
    "# target_metric = 'loss_low_suspension' #\"loss_very_low_suspension\", \"loss_low_suspension\", \"loss_medium_suspension\", \"loss_high_suspension\", \"loss_CoC_values\", \"loss_EPA_values\"\n",
    "# higher_is_better = False\n",
    "\n",
    "# algorithm = 'efficiencymap'\n",
    "# target_metric = 'rmse' # 'elapsed_time', 'num_total_data_points', 'num_valid_data_points', 'num_removed', 'num_removed_neg_torque', 'num_removed_soc', 'rmse_diff', 'mean_abs_diff'\n",
    "# higher_is_better = False\n",
    "# select_gear = '1' # None, '1', '2', 'vw'\n",
    "\n",
    "# algorithm = 'gearstrategy'\n",
    "# target_metric = 'prmse_global' # hd_global, dtw_global, cd_global, fd_global, prmse_global, fd_normal, fd_sport, etc., fd_rmt_global\n",
    "# higher_is_better = False\n",
    "\n",
    "\n",
    "# load log\n",
    "if algorithm == 'coastdown':\n",
    "    log_path = 'data/logs/coastdown_log.csv'\n",
    "    select_gear = None\n",
    "elif algorithm == 'constspeed':\n",
    "    log_path = 'data/logs/constspeed_log.csv'\n",
    "    select_gear = None\n",
    "elif algorithm == 'efficiencymap':\n",
    "    log_path = 'data/logs/efficiencymap_log.csv'\n",
    "elif algorithm == 'gearstrategy':\n",
    "    log_path = 'data/logs/gear_strategy_log.csv'\n",
    "    select_gear = None\n",
    "\n",
    "results_df = pd.read_csv(log_path)\n",
    "\n",
    "# fix windows file paths\n",
    "if 'files' in results_df.columns:\n",
    "    results_df['files'] = results_df['files'].str.replace('\\\\\\\\', '/')\n",
    "    results_df['files'] = results_df['files'].str.replace('\\\\', '/')\n",
    "if 'gear' in results_df.columns:\n",
    "    results_df['gear'] = results_df['gear'].astype(str)\n",
    "\n",
    "print(f'Total runs: {len(results_df)}')\n",
    "results_df.value_counts('comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results space filtering\n",
    "\n",
    "Currently, all grid search runs that are present are loaded. Now a specific run can be selected by inputing the desired comment into `specific_comment`. If set to None, the last run will be selected automatically. If a specific number of recent runs should be selected, the `number_of_recents` parameter can be set to something other than None. Lastly, if all runs from the log file should be selected, the `select_all` parameter can be set to True.\n",
    "\n",
    "Additionally, a `limit` can be supplied to filter out runs that went extremely bad to avoid skewing the subsequent analysis in undesired ways.  The limit should be chosen at least 10 times the expected value of the target metric, to avoid filtering out good runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter options - set to None to automatically select the most recent run\n",
    "specific_comment = None\n",
    "number_of_recents = None\n",
    "\n",
    "select_all = False\n",
    "limit = 200\n",
    "\n",
    "\n",
    "# Filtering logic\n",
    "if limit is not None:\n",
    "    if higher_is_better:\n",
    "        results_df = results_df[results_df[target_metric] >= limit]\n",
    "    else:\n",
    "        results_df = results_df[results_df[target_metric] <= limit]\n",
    "if select_gear is not None:\n",
    "    results_df = results_df[results_df['gear'] == select_gear]\n",
    "if specific_comment is not None:\n",
    "    results_df = results_df[results_df['comment'] == specific_comment]\n",
    "elif number_of_recents is not None:\n",
    "    results_df = results_df.iloc[-number_of_recents:]\n",
    "elif select_all:\n",
    "    pass\n",
    "else:\n",
    "    results_df = results_df[results_df['comment'] == results_df['comment'].iloc[-1]]\n",
    "\n",
    "print(f'Selected {len(results_df)} runs with comment: {results_df[\"comment\"].iloc[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## First look at the data\n",
    "\n",
    "This function scans the results space to automatically detect which parameters were varied in the grid search. It also extracts the run with the lowest target metric value, which can later be used to run the pipeline with these parameters. Also a histogram of the target metric values is shown to get a first impression of the results space.\n",
    "\n",
    "The `varied_parameters` list contains all the design parameters that will be used in the subsequent analysis. If any parameters should be excluded, they can be removed from this list here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.parametric_pipelines import evaluate_results\n",
    "\n",
    "varied_parameters, best_combination = evaluate_results(results_df, target_metric, algorithm, higher_is_better)\n",
    "\n",
    "# remove non-parameters\n",
    "varied_parameters = [param for param in varied_parameters if param != 'gear' and param != 'files' and param != 'columns_to_smooth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Statisical significance testing\n",
    "\n",
    "To test if the design parameters (IVs) have a significant impact on the target metric (DV), a statistical significance test is performed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Distinguish categorical vs numeric parameters\n",
    "categorical_params = [p for p in varied_parameters if results_df[p].dtype == object or results_df[p].dtype == bool]\n",
    "numeric_params = [p for p in varied_parameters if p not in categorical_params]\n",
    "\n",
    "# Construct the formula for the linear model\n",
    "# C() indicates categorical variables. Numeric variables are included as-is.\n",
    "formula_terms = []\n",
    "for p in numeric_params:\n",
    "    formula_terms.append(f\"{p}\")  # numeric variables included directly\n",
    "for p in categorical_params:\n",
    "    formula_terms.append(f\"C({p})\")  # categorical variables indicated with C()\n",
    "formula = f\"{target_metric} ~ \" + \" + \".join(formula_terms)\n",
    "\n",
    "print(\"Fitting model with formula:\")\n",
    "print(formula)\n",
    "\n",
    "# Fit the model using OLS\n",
    "model = smf.ols(formula=formula, data=results_df).fit()\n",
    "\n",
    "# Perform ANOVA on the fitted model\n",
    "anova_results = sm.stats.anova_lm(model, typ=2)  # Type-II ANOVA\n",
    "\n",
    "print(\"ANOVA results:\")\n",
    "print(anova_results)\n",
    "from modules.plotter import plot_anova_bar\n",
    "plot_anova_bar(anova_results, color='#0065bd', name='testing', figsize=(10,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pairplot investigation\n",
    "\n",
    "To identify parameter interactions, a pairplot is generated. The list `params_to_plot` can be adjusted easily here. Default behaviour is to use `varied_parameters` from the first look at the data. Depending on the length of the list, different approprate plots are generated. If the list is longer than two, another sensitivity analysis plot is generated. This sensitivity analysis plot makes use of the random forests ability to provide feature importances. After training a random forest  to regress the target metric, the feature importances are used to plot the sensitivity of the target metric to the different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.plotter import plot_distribution_against_hyperparameter, threeDplot, plot_hyperparameter_heatmaps, plot_parameter_importance_analysis\n",
    "\n",
    "# Plot the results\n",
    "params_to_plot = [param for param in varied_parameters if param != 'files']\n",
    "#params_to_plot = ['soc_limit_lower', 'soc_limit_upper']\n",
    "\n",
    "if len(params_to_plot) == 1:\n",
    "    plot_distribution_against_hyperparameter(results_df, params_to_plot, target_metric, figsize=(12, 8))\n",
    "elif len(params_to_plot) == 2:\n",
    "    threeDplot(results_df, params_to_plot, target_metric, figsize=(800,800), higher_is_better=higher_is_better)\n",
    "else:\n",
    "    plot_hyperparameter_heatmaps(results_df, params_to_plot, target_metric, figsize=(15,15), higher_is_better=higher_is_better, use_identical_scale=False, aggregation='mean')\n",
    "    plot_parameter_importance_analysis(results_df, params_to_plot, target_metric, figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Individual parameter insights\n",
    "\n",
    "This widget allows to scroll through the list of parameters to output the aggregated results that contain the specific value of the selected hyperparameter as a box plot. Additional information is provided, by printing which distribution has the lowest median target metric value, the lowest variance, and the lowest interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# optionally enforce axis limits\n",
    "axis_limits = {\n",
    "    'y': [0, 50]\n",
    "} \n",
    "\n",
    "from modules.plotter import plot_distribution_against_hyperparameter\n",
    "# Define the interactive widget function\n",
    "def interactive_plot(param_index):\n",
    "    param = varied_parameters[param_index]\n",
    "    param = [param]\n",
    "    if axis_limits is not None:\n",
    "        plot_distribution_against_hyperparameter(results_df, param, target_metric, figsize=(8, 6), axis_limits=axis_limits)\n",
    "    else:\n",
    "        plot_distribution_against_hyperparameter(results_df, param, target_metric, figsize=(8, 6))\n",
    "\n",
    "# Create the slider\n",
    "param_slider = widgets.IntSlider(value=0, min=0, max=len(varied_parameters)-1, step=1, description='Parameter')\n",
    "widgets.interactive(interactive_plot, param_index=param_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run pipeline with \"best\" parameters\n",
    "\n",
    "Here the pipeline is run with the parameters which showed the \"best\" target metric. This has to be used with caution, as the lowest target metric value might not be the best choice. This is mostly for getting a quick overview if the pipeline is prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.parametric_pipelines import run_pipeline_with_best_parameters\n",
    "run_pipeline_with_best_parameters(best_combination, algorithm, generate_plots=True, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
